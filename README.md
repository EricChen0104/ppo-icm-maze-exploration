# Curiosity-Driven Rescue: PPO + ICM Agent for Maze Exploration

> ğŸš¨ A smart agent navigating unknown mazes to rescue victims â€” guided by curiosity, not just rewards.

This project implements a **Proximal Policy Optimization (PPO)** agent augmented with an **Intrinsic Curiosity Module (ICM)**, trained to autonomously explore a maze-like environment, find hidden victims, and learn efficient navigation strategies through internal motivation.

### DEMO
![]()

---

## ğŸ¯ Objectives

The goal is to train an intelligent agent that can:

- Efficiently explore an unknown 2D maze
- Avoid getting stuck in dead ends or revisiting known areas
- Quickly react to visible victims and rescue them

The project is designed as a stepping stone toward full **SLAM-based autonomous exploration** systems in robotics.

---

## ğŸ§  Core Techniques

| Module        | Method                     | Description                                       |
|---------------|----------------------------|---------------------------------------------------|
| Reinforcement Learning | PPO        | Stable, widely used policy gradient algorithm     |
| Exploration    | ICM        | Curiosity-driven intrinsic rewards based on prediction error |
| Temporal Memory| LSTM       | Helps the agent remember explored vs unexplored areas |
| Vision Input   | CNN        | Encodes local observations                        |
| Mapping        | Occupancy Grid Map (OGM) | Supports frontier-based planning and SLAM integration |

---

## ğŸ—ºï¸ Environment Highlights â€“ `RescueGridEnv`

- **Grid-based environment**: customizable size, obstacle density, and visibility radius
- **Partial observability**: local vision to simulate real-world sensing
- **Dynamic victim locations**: randomized at every episode
- **Visualization**: real-time rendering with `pygame`
- **Exploration tracking**: supports visited map and pseudo-count reward shaping

---

## ğŸ” Performance Snapshot

| Agent            | Avg. Explored | Success Rate | Avg. Reward |
|------------------|---------------|---------------|-------------|
| PPO (Baseline)   | 46.6%         | 0.00%         | âˆ’223.59     |
| PPO + ICM        | 47.4%         | 0.00%         | âˆ’207.57     |

> âš ï¸ Although ICM improves early exploration marginally, both agents struggle with long-term planning. Upcoming work includes frontier-based shaping and hierarchical control.

---

## ğŸš€ How to Run

```bash
# Install dependencies
pip install -r requirements.txt

# Train PPO + ICM
python main.py

# Test trained agent
python test.py
```

## ğŸ“ Project Structure
```bash
MAZE_RESCUE/
â”œâ”€â”€ main.py               # Training entry point
â”œâ”€â”€ test.py               # Testing script
â”œâ”€â”€ PPO_agent.py          # PPO and PPO+ICM implementations
â”œâ”€â”€ ICM.py                # Intrinsic Curiosity Module
â”œâ”€â”€ actor_critic_cnn.py   # CNN + LSTM-based network
â”œâ”€â”€ rescue_env.py         # Custom GridWorld rescue environment
â”œâ”€â”€ assets/               # Visualizations and logs
â””â”€â”€ model/                # Saved models
```

ğŸ“ˆ What's Next
- âœ… Frontier detection + potential-based reward shaping
- âœ… Hierarchical RL (Manager-Worker subgoal architecture)
- âœ… Transition to continuous action space (v, Ï‰)
- âœ… SLAM integration with EKF or Particle Filter
- âœ… Publication to competitions, exhibitions, or academic venues


## ğŸ™‹ About the Author
This project was developed by a high school student passionate about AI, reinforcement learning, and robotics. The long-term vision is to build a quadruped robot that can explore real-world environments, build maps, and locate victims â€” all learned through intelligent curiosity.

Feel free to â­ï¸ the repo, fork it, or reach out if you want to collaborate!

